
# ü¶¶ Instalando e Usando Ollama na VPS Ubuntu

Este guia descreve os passos para instalar o Ollama em uma VPS (Virtual Private Server) rodando Ubuntu, e como baixar e gerenciar modelos de linguagem. Ollama permite que voc√™ execute modelos de linguagem grandes (LLMs) localmente.

## üí° Considera√ß√µes Iniciais para VPS

- **Recursos da VPS:** Modelos de linguagem podem ser exigentes.
  - **RAM:** Para uma VPS com 4GB de RAM, voc√™ estar√° limitado a modelos menores (ex.: ~3 bilh√µes de par√¢metros como Phi-3 Mini, Gemma:2b). Modelos maiores exigir√£o significativamente mais RAM.
  - **Espa√ßo em Disco:** Cada modelo ocupa alguns gigabytes de espa√ßo. Certifique-se de ter espa√ßo suficiente.
  - **CPU:** Um bom processador ajudar√° na velocidade de infer√™ncia.
- **Acesso Root/Sudo:** Voc√™ precisar√° de privil√©gios `sudo` para a instala√ß√£o.

---

## ‚öôÔ∏è 1. Instala√ß√£o do Ollama

O Ollama fornece um script de instala√ß√£o que simplifica o processo.

### 1.1. Baixar e Executar o Script de Instala√ß√£o

O comando a seguir baixa e executa o script de instala√ß√£o oficial do Ollama:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Este script detectar√° automaticamente o seu sistema operacional e arquitetura. Ele tamb√©m tentar√° configurar o Ollama para usar uma GPU NVIDIA, se detectada e corretamente configurada (veja a se√ß√£o sobre Suporte a GPU).

### 1.2. Verificar a Instala√ß√£o

Ap√≥s a conclus√£o do script, verifique se o Ollama foi instalado corretamente:

```bash
ollama --version
```

Isso deve exibir a vers√£o instalada do Ollama.

---

## ‚öôÔ∏è 2. Gerenciando o Servi√ßo Ollama

O script de instala√ß√£o geralmente configura o Ollama para rodar como um servi√ßo `systemd`.

### 2.1. Verificar o Status do Servi√ßo

```bash
sudo systemctl status ollama
```

Voc√™ deve ver que o servi√ßo est√° **"active (running)"**.

### 2.2. Comandos Comuns do Servi√ßo (se necess√°rio)

- **Iniciar o servi√ßo Ollama:**

```bash
sudo systemctl start ollama
```

- **Parar o servi√ßo Ollama:**

```bash
sudo systemctl stop ollama
```

- **Habilitar o Ollama para iniciar no boot:**

```bash
sudo systemctl enable ollama
```

- **Desabilitar o Ollama de iniciar no boot:**

```bash
sudo systemctl disable ollama
```

- **Reiniciar o servi√ßo (ap√≥s alguma configura√ß√£o, por exemplo):**

```bash
sudo systemctl restart ollama
```

---

## üöÄ 3. Baixando e Gerenciando Modelos

Com o Ollama instalado e o servi√ßo rodando, voc√™ pode baixar e executar modelos.

### 3.1. Baixar um Modelo (Pull)

Para baixar um modelo sem execut√°-lo imediatamente, use `ollama pull`. Voc√™ pode encontrar uma lista de modelos dispon√≠veis na biblioteca Ollama.

- Por exemplo, para baixar o modelo **phi3:mini** (um modelo menor, adequado para sistemas com menos RAM):

```bash
ollama pull phi3:mini
```

- Ou um modelo um pouco maior, como **gemma:2b**:

```bash
ollama pull gemma:2b
```

### 3.2. Executar um Modelo Interativamente (Run)

Para baixar (se ainda n√£o baixado) e executar um modelo interativamente no terminal:

```bash
ollama run phi3:mini
```

Isso abrir√° um prompt onde voc√™ pode conversar com o modelo. Para sair, digite `/bye`.

### 3.3. Listar Modelos Baixados Localmente

Para ver todos os modelos que voc√™ j√° baixou:

```bash
ollama list
```

### 3.4. Remover um Modelo Local

Se precisar liberar espa√ßo, voc√™ pode remover um modelo:

```bash
ollama rm <nome_do_modelo>
```

Exemplo:

```bash
ollama rm phi3:mini
```

---

## üî• 4. Acessando a API do Ollama Remotamente (Opcional)

Por padr√£o, o Ollama serve sua API na porta **11434** e escuta em **localhost**. Se voc√™ deseja que a API seja acess√≠vel de outras m√°quinas (ex.: da sua m√°quina local para a VPS), voc√™ precisar√°:

### 4.1. Configurar o Ollama para Escutar em Todos os IPs

Edite o servi√ßo systemd do Ollama para definir a vari√°vel de ambiente `OLLAMA_HOST`.

- Crie um arquivo de override para o servi√ßo:

```bash
sudo systemctl edit ollama.service
```

- Adicione o seguinte conte√∫do no editor que abrir, salve e feche:

```ini
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
```

- Recarregue o systemd e reinicie o Ollama:

```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### 4.2. Abrir a Porta no Firewall da VPS (se estiver usando ufw)

```bash
sudo ufw allow 11434/tcp
sudo ufw reload
sudo ufw status
```

Agora voc√™ deve conseguir acessar a API do Ollama de outra m√°quina usando:

```
http://<IP_DA_SUA_VPS>:11434
```

‚ö†Ô∏è **Aten√ß√£o:** Expor a API publicamente sem medidas de seguran√ßa adicionais pode ser um risco. Considere usar um firewall para restringir o acesso apenas aos seus IPs, ou configurar um proxy reverso com autentica√ß√£o.

---

## üîå 5. (Opcional) Suporte a GPU NVIDIA

Se sua VPS possui uma GPU NVIDIA compat√≠vel, o Ollama pode utiliz√°-la para uma infer√™ncia muito mais r√°pida.

### 5.1. Pr√©-requisitos

- **Drivers NVIDIA:** Os drivers NVIDIA apropriados devem estar instalados.
- **NVIDIA Container Toolkit:** Embora o Ollama n√£o rode em cont√™ineres por padr√£o, suas depend√™ncias de GPU s√£o similares.

O script de instala√ß√£o:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

tentar√° detectar automaticamente uma GPU NVIDIA e seus drivers. Se bem-sucedido, o Ollama usar√° a GPU.

### 5.2. Verifica√ß√£o

Ao executar um modelo, o Ollama geralmente indica se est√° usando a GPU. Voc√™ tamb√©m pode monitorar o uso da GPU com:

```bash
nvidia-smi
```

em outro terminal enquanto um modelo est√° carregado ou processando.

Se voc√™ instalar os drivers NVIDIA **ap√≥s** ter instalado o Ollama, pode ser necess√°rio **reinstalar o Ollama** ou garantir que as bibliotecas CUDA estejam acess√≠veis para ele.

---

Se quiser, posso gerar esse guia em PDF ou HTML pra voc√™. Quer?
